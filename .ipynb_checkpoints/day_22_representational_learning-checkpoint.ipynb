{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recap\n",
    "We use the latent vector as 'representations' of the original information, basically a compression of the information into the relevant features.\n",
    "\n",
    "We can use these for example in recommender systems as they are fast and allow for an easier measure of the distance (e.g. euclidian measure, vector angles, binary cross-entropy, ...) or nearest neighbours on the actual content instead of e.g. just the pixels (example: a picture is rotated, but content is preserved in the latent vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an encoder, we need to train the full auto-encoder (encoder + decoder) using the same input and output; once we're happy with the auto-encoder we throw away the decoder and have an encoder from which we know that it is a good representation of the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Fake:\n",
    "We train two auto-encoders, one for person A and one for person B. Once trained, we replace the decoder of person A with the one of person B. As we have the same latent space (the encoders have the same structure), we can replace the face of person A with person B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
