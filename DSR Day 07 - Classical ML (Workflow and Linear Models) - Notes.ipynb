{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "* Start locally with Jupyter Notebook and a small data set\n",
    "* Move to cloud with Jupyter Notebook and go for full data set\n",
    "* Do in a proper script with distributed computing\n",
    "\n",
    "Reading note: Netflix blog entry for their data science workflow.\n",
    "\n",
    "Data engineering vs. data scientist: it's good if the data scientist can also do e.g. the data cleaning (what typically the data engineer is doing). This reduces cycle time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Tricks\n",
    "\n",
    "**Connecting to a Jupyter notebook to a server hosted on a server**\n",
    "ssh --CL localhost:1337:localhost:1337 -L localhost:1338:localhost:1338 scloudera01\n",
    "\n",
    "Renting a server and running the Jupyter on the server would be a good idea (makes running the scripts faster). There are 'commodity' and 'tesla' - the latter have error correction and 32bit precision. This is not important for ANN, and 'tesla' is 4x as expensive.\n",
    "\n",
    "**On the shell:**\n",
    "* We normally use 'python my_exercise.py' (then we can't run anything else)\n",
    "* Use the & when executing 'python my_exercise.py &' (but it will die if we log out)\n",
    "* Even better: 'nohup python my_exercise.py' then it continues running even if we log out). The output goes to a nohup.out file.\n",
    "\n",
    "\n",
    "**Command line**\n",
    "* Use top / htop to show CPU usage; does not work on Windows (except maybe in PowerShell)\n",
    "* Maybe read book \"Data science in the command line\"\n",
    "\n",
    "**Jupyter Notebook**\n",
    "* Ctrl + Shift + P to open the list of commands\n",
    "* Ctrl + Shift + Minus to split a cell in two at cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Five approaches / schools:**\n",
    "* Connectionist: re-model the brain (back-propagation, stochastic gradient descent)\n",
    "* Evolutionist (deep learning): neural network solutions; computation time is the limiting factor.\n",
    "* Bayesian: problem is that it's difficult to model, might be computational expensive to run (typically done with Monte Carl); it's about detecting trends.\n",
    "* Analogists: try to deduce from the existing (e.g. SVM, KNN (k-nearest-neighbour)). Statistical learning.\n",
    "* Symbolists: logic approach, one finds ontologies.\n",
    "    \n",
    "Deep learning is so successful because it also enables transfer learning (e.g. edge detection, face detection) and we can adapt the model from a good basis already. \n",
    "\n",
    "Main reasons why ML is taking off:\n",
    "* Bigger training sets\n",
    "* Faster and specialized hardware\n",
    "* Open source tools\n",
    "* Improved algorithms\n",
    "\n",
    "Induction bias: example of diminishing returns. If we don't account for that, the model would predict steady returns (if we e.g. increase the budget for ads massively). Other induction biases are e.g. due to race. One needs to be conscious of these biases. Testing this is with e.g. testing what effect an increase would have or how the model would behave if we switch race. If the underlying data changes, we need to apply judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Work\n",
    "\n",
    "When a data scientist starts in a company, the first step is often setting up infrastructure. Then finding the data. Then 80% of the time is finding the correct data representation (where data cleaning is part of) such that it's in a ML-compatible format. Difficult e.g. for pictures with differing sizes. Various ways to do that, e.g. reduction to only colors.\n",
    "\n",
    "Feature learning is the classical skill of a data scientist; this gets less important with DL as it does the 'feature embedding'. We could e.g. build a random forest on top of a neural net. This is not done often (learning on the features isolated by the NN). Back-propagation is the word here.\n",
    "\n",
    "Another factor which is very hard is to bring down the computation time. Correcting single features can have a huge impact. There often is a mix between automated and manual feature extraction (when not being in DL). Feature engineering often starts with heuristics. For model performance evaluation we use hypothesis and t-tests to assess its significance. \n",
    "\n",
    "We always need to have a baseline. What is a good improvement depends very much on the problem at hand. Usually there are diminishing returns. This is typically a cost-benefit-decision (e.g. automated classification of insurance claims into accept / look at manually / reject in terms of false claims benefits, admin costs or potential legal costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "_Website Visitor Example:_\n",
    "Website crawling in combination with 'bag of words', then clustering (then use silhoutte and elbow analysis). Dimensionality reduction for analysis (e.g. use principal component analysis (PCA)) as often it's more than three dimensions. TSNE = t-distributed stochastic neighbor embedding can also be used. TSNE is used to project down from multiple dimensions to only two. The axis are difficult to interprete though.\n",
    "\n",
    "_Voting Behavior of MP in Bundestag_\n",
    "Check uberwach.github.io/visualisierung-der-abgeordneten.html The x-axis shows 'alignment to government policies'; the two axes are the principle components. See also difference between PCA and TSNE. In the TSNE the clustering is done for 'the more similar people vote' (but it's not interpretable otherwise). \n",
    "\n",
    "_Otto Competition in Kaggle_\n",
    "This helps in identifying which product classes are easy to classify and which ones not.\n",
    "\n",
    "_Latent Dirichlet Allocation_\n",
    "Shows the classification of e.g. a text to categories (e.g. Cristiano Ronaldo is 40% football and 60% celebrity). It's a graphical model, but not easy to interprete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "#### k-nearest-neighbour\n",
    "Prone to learn a problem by heart (overfitting). It's used for classification. In general one should not use it as almost always there are better models. A variant is called _k-similar-neigbour_ where we have a similarity measure which can be used to discern features.\n",
    "\n",
    "#### Non-parametric models\n",
    "Models where the number of parameters grow with the data. Problems: we have too keep all instances in memory, slow inference and curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models\n",
    "Linear models show how machine learner think. They are easy to tune and scalable.\n",
    "\n",
    "The f we calculate has basically the form of theta transposed * x. The weights are in stored in theta (which has the same dimensionality as the X). It's then a vector multiplication (theta transposed * x). Hyperparameters adapt the optimizer which calculates theta. Problem of hyperparameter search: can be done by trying out, by brute force, randomized search, grid search, evolutionary search or Bayesian hyperparameter search (but not used in practice).\n",
    "\n",
    "Regularization (i.e. restricting the classes of functions f(x) = y) reduces the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example banana / apple / red banana: the learner should need to learn both shape and color, and then one of them should be weighted more. Ideally the model has a probability-weighted prediction (e.g. 70% probability that it's an apple, i.e. a lower confidence).\n",
    "\n",
    "Use cross-validation to increase generalization / repeatability of model.\n",
    "\n",
    "Difference between deterministic and stochastic gradient descent. \n",
    "\n",
    "We have a loss function we want to minimize; stochastic optimization is done in each step (i.e. for each x). The optimization the converges. In NN we do not use single steps, but mini-batches. Larger batches helps in regularization (i.e. reducing overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Squares\n",
    "We regularize with e.g. L2 (adding a lambda \\* (w1^2 + ... + wd^2). This leads to a different loss. We optimize for the lambda (which is a hyperparameter). The higher lambda is, the less the data matters (full formula is sum(yi - w^t * xi)^2 + lambda \\* (w1^2 + ... + wd^2). How do we set lambda? By the way, in scikit-learn lambda is called alpha. Optimization is done on a log scale.\n",
    "\n",
    "Outliers: in general we don't throw data away. It depends is it an error (e.g. measurement error) or is there a causal relationship? If it's the former: remove it. If it's the latter: use regularization to deal with it. In big data \n",
    "\n",
    "Optimization: sometimes it can be better to start with a few features and gradually add more; sometimes better start with all features and gradually remove noisy features.\n",
    "\n",
    "Lasso (L1) selection (abs instead of square) presses weights to zero and leads to sparsity of features (i.e. weights and thus features get removed). L2 is more smooth. In general L1 is better.\n",
    "\n",
    "Elastic net: adding an additional parameter tau in [0, 1] which is applied to both L1 and L2 (once with tau, once with (1 - tau)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slide 77: the x_n is there because sometimes the loss depends on x_n (example: gold nuggets. the loss is much higher if the stone (potential nugget) is larger. This needs to be weighted in terms of wrongly classifying a stone as a nugget and wrongly classifying a nugget as a stone).\n",
    "\n",
    "Sometimes we also weight by the label.\n",
    "\n",
    "We are not allowed to use the training data for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slide 80\n",
    "**Model fitting:**: finding the best model parameter (weights). This is solving an optimization, should be robust and stable). \n",
    "Training data is used; returns 45 models in the case of the polynomial example.\n",
    "\n",
    "**Model selection:**\n",
    "Validation data is used. We select the best of the 45. Then we re-train that model on the train + validation data (using the selected hyperparameters).\n",
    "\n",
    "**Model evaluation:**\n",
    "Test data is used to check the model.\n",
    "\n",
    "Slide 86\n",
    "CV = cross-validation\n",
    "Test instance = validation data set; however, it could also be done with the test data (then it's 'nested cross-validation', see slide 88)\n",
    "Normally we use k=5 or k=10\n",
    "\n",
    "Slide 87\n",
    "LOOCV = Leave-one-out cross-validation\n",
    "\n",
    "We use nested CV with low data, high compute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
