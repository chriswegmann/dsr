{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CODE_DIR = '/home/shreyas/Documents/git/wheelai/'\n",
    "#DATA_DIR = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/'\n",
    "DATA_DIR = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/sample/'\n",
    "traindata_path = DATA_DIR + 'train/'\n",
    "validdata_path = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/valid/'\n",
    "results_path = '/media/shreyas/OS/ML_DATA/wheelai/kerakart/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16\n",
    "\n",
    "%matplotlib inline\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(path, class_mode='categorical', gen=image.ImageDataGenerator(), \\\n",
    "                shuffle=True, target_size=(224,224), batch_size=1):\n",
    "    return gen.flow_from_directory(path, class_mode=class_mode, batch_size=batch_size, \\\n",
    "                                   target_size=target_size, shuffle=shuffle)\n",
    "\n",
    "def get_steps(batches, batch_size):\n",
    "    steps = int(batches.samples/batch_size)\n",
    "    return (steps if batches.samples%batch_size==0 else (steps+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_b = get_batches(traindata_path, batch_size=32)\n",
    "valid_b = get_batches(validdata_path, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = train_b.classes\n",
    "valid_labels = valid_b.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(train_labels)\n",
    "y_valid = keras.utils.to_categorical(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000,) (2000,) (23000, 2) (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape, valid_labels.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_conv():\n",
    "    conv_model = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "    return conv_model\n",
    "\n",
    "\n",
    "def top_layer(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_b = get_batches(traindata_path, class_mode=None, shuffle=False, batch_size=batch_size)\n",
    "val_b = get_batches(validdata_path, class_mode=None, shuffle=False, batch_size=batch_size)\n",
    "tst_b = get_batches(testdata_path, class_mode=None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_augdata = image.ImageDataGenerator(rotation_range=40, \n",
    "                                      width_shift_range=.2, \n",
    "                                      height_shift_range=.2, \n",
    "                                      shear_range=0.2,\n",
    "                                      zoom_range=0.2, \n",
    "                                      horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "augtrain_b = get_batches(traindata_path, gen=gen_augdata, class_mode='categorical', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_class = trn_b.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_steps = get_steps(trn_b, batch_size)\n",
    "val_steps = get_steps(val_b, batch_size)\n",
    "tst_steps = get_steps(tst_b, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg_conv = vgg_conv()\n",
    "vgg_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "trn_ft = vgg_conv.predict_generator(trn_b, trn_steps)\n",
    "val_ft = vgg_conv.predict_generator(val_b, val_steps)\n",
    "#tst_ft = vgg_conv.predict_generator(tst_b, tst_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trn_ft.shape, val_ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(results_path + 'trn_ft.dat', trn_ft)\n",
    "save_array(results_path + 'val_ft.dat', val_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = load_array(results_path + 'trn_ft.dat')\n",
    "X_valid = load_array(results_path + 'val_ft.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = top_layer(X_train.shape[1:])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(results_path+'bottleneck_topmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = vgg_conv()\n",
    "for layer in base_model.layers[:15]: layer.trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_model = top_layer(base_model.output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_model.load_weights(results_path+'bottleneck_topmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = top_model(base_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg_model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = results_path+'ft01.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True,\\\n",
    "                             save_weights_only=True, mode='min', period=1)\n",
    "callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9922Epoch 00000: val_loss improved from inf to 0.14034, saving model to /media/shreyas/OS/ML_DATA/dogsvscats/results/ft01.h5\n",
      "719/719 [==============================] - 169s - loss: 0.0204 - acc: 0.9922 - val_loss: 0.1403 - val_acc: 0.9695\n",
      "Epoch 2/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9932Epoch 00001: val_loss did not improve\n",
      "719/719 [==============================] - 166s - loss: 0.0205 - acc: 0.9931 - val_loss: 0.1532 - val_acc: 0.9680\n",
      "Epoch 3/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9882Epoch 00002: val_loss did not improve\n",
      "719/719 [==============================] - 167s - loss: 0.0343 - acc: 0.9881 - val_loss: 0.1663 - val_acc: 0.9600\n",
      "Epoch 4/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9763Epoch 00003: val_loss did not improve\n",
      "719/719 [==============================] - 167s - loss: 0.0719 - acc: 0.9761 - val_loss: 0.4055 - val_acc: 0.9250\n",
      "Epoch 5/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9796Epoch 00004: val_loss did not improve\n",
      "719/719 [==============================] - 168s - loss: 0.0607 - acc: 0.9797 - val_loss: 0.1711 - val_acc: 0.9555\n",
      "Epoch 6/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "719/719 [==============================] - 169s - loss: 0.0426 - acc: 0.9856 - val_loss: 0.1492 - val_acc: 0.9625\n",
      "Epoch 7/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9814Epoch 00006: val_loss did not improve\n",
      "719/719 [==============================] - 167s - loss: 0.0544 - acc: 0.9814 - val_loss: 4.2274 - val_acc: 0.5635\n",
      "Epoch 8/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9525Epoch 00007: val_loss did not improve\n",
      "719/719 [==============================] - 167s - loss: 0.1549 - acc: 0.9525 - val_loss: 2.9675 - val_acc: 0.6395\n",
      "Epoch 9/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.9476Epoch 00008: val_loss did not improve\n",
      "719/719 [==============================] - 167s - loss: 0.1624 - acc: 0.9475 - val_loss: 0.1718 - val_acc: 0.9675\n",
      "Epoch 10/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9640Epoch 00009: val_loss improved from 0.14034 to 0.10346, saving model to /media/shreyas/OS/ML_DATA/dogsvscats/results/ft01.h5\n",
      "719/719 [==============================] - 159s - loss: 0.1085 - acc: 0.9640 - val_loss: 0.1035 - val_acc: 0.9695\n",
      "Epoch 11/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8011Epoch 00010: val_loss did not improve\n",
      "719/719 [==============================] - 152s - loss: 0.4806 - acc: 0.8012 - val_loss: 0.8290 - val_acc: 0.7790\n",
      "Epoch 12/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8597Epoch 00011: val_loss did not improve\n",
      "719/719 [==============================] - 154s - loss: 0.3531 - acc: 0.8596 - val_loss: 0.4068 - val_acc: 0.8700\n",
      "Epoch 13/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.3871 - acc: 0.8386Epoch 00012: val_loss did not improve\n",
      "719/719 [==============================] - 154s - loss: 0.3869 - acc: 0.8386 - val_loss: 0.6081 - val_acc: 0.8130\n",
      "Epoch 14/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.8784Epoch 00013: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.3097 - acc: 0.8785 - val_loss: 0.3428 - val_acc: 0.8735\n",
      "Epoch 15/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.2655 - acc: 0.9016Epoch 00014: val_loss did not improve\n",
      "719/719 [==============================] - 156s - loss: 0.2657 - acc: 0.9015 - val_loss: 0.2813 - val_acc: 0.9410\n",
      "Epoch 16/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9132Epoch 00015: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.2408 - acc: 0.9132 - val_loss: 0.2092 - val_acc: 0.9455\n",
      "Epoch 17/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9275Epoch 00016: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.2090 - acc: 0.9276 - val_loss: 0.2066 - val_acc: 0.9495\n",
      "Epoch 18/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9266Epoch 00017: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.2129 - acc: 0.9267 - val_loss: 0.2043 - val_acc: 0.9415\n",
      "Epoch 19/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9322Epoch 00018: val_loss did not improve\n",
      "719/719 [==============================] - 160s - loss: 0.1902 - acc: 0.9322 - val_loss: 0.2908 - val_acc: 0.9025\n",
      "Epoch 20/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9365Epoch 00019: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.1862 - acc: 0.9365 - val_loss: 0.1773 - val_acc: 0.9515\n",
      "Epoch 21/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9398Epoch 00020: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.1687 - acc: 0.9399 - val_loss: 0.1711 - val_acc: 0.9410\n",
      "Epoch 22/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1570 - acc: 0.9473Epoch 00021: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.1569 - acc: 0.9474 - val_loss: 0.1565 - val_acc: 0.9590\n",
      "Epoch 23/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1718 - acc: 0.9403Epoch 00022: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.1718 - acc: 0.9404 - val_loss: 0.2012 - val_acc: 0.9550\n",
      "Epoch 24/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9449Epoch 00023: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.1576 - acc: 0.9448 - val_loss: 0.1660 - val_acc: 0.9555\n",
      "Epoch 25/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9497Epoch 00024: val_loss did not improve\n",
      "719/719 [==============================] - 156s - loss: 0.1450 - acc: 0.9497 - val_loss: 0.1393 - val_acc: 0.9595\n",
      "Epoch 26/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.9516Epoch 00025: val_loss did not improve\n",
      "719/719 [==============================] - 159s - loss: 0.1391 - acc: 0.9516 - val_loss: 0.1173 - val_acc: 0.9660\n",
      "Epoch 27/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1322 - acc: 0.9556Epoch 00026: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.1324 - acc: 0.9556 - val_loss: 0.1043 - val_acc: 0.9710\n",
      "Epoch 28/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1238 - acc: 0.9564Epoch 00027: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.1238 - acc: 0.9564 - val_loss: 0.3378 - val_acc: 0.9265\n",
      "Epoch 29/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9568Epoch 00028: val_loss did not improve\n",
      "719/719 [==============================] - 159s - loss: 0.1250 - acc: 0.9568 - val_loss: 0.1148 - val_acc: 0.9655\n",
      "Epoch 30/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9600Epoch 00029: val_loss improved from 0.10346 to 0.09715, saving model to /media/shreyas/OS/ML_DATA/dogsvscats/results/ft01.h5\n",
      "719/719 [==============================] - 158s - loss: 0.1194 - acc: 0.9599 - val_loss: 0.0972 - val_acc: 0.9675\n",
      "Epoch 31/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1136 - acc: 0.9606Epoch 00030: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.1136 - acc: 0.9606 - val_loss: 0.1021 - val_acc: 0.9715\n",
      "Epoch 32/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9628Epoch 00031: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.1098 - acc: 0.9629 - val_loss: 0.1004 - val_acc: 0.9710\n",
      "Epoch 33/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1061 - acc: 0.9643Epoch 00032: val_loss did not improve\n",
      "719/719 [==============================] - 159s - loss: 0.1065 - acc: 0.9641 - val_loss: 0.1017 - val_acc: 0.9665\n",
      "Epoch 34/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1029 - acc: 0.9648Epoch 00033: val_loss did not improve\n",
      "719/719 [==============================] - 154s - loss: 0.1028 - acc: 0.9649 - val_loss: 0.1194 - val_acc: 0.9715\n",
      "Epoch 35/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9617Epoch 00034: val_loss did not improve\n",
      "719/719 [==============================] - 156s - loss: 0.1107 - acc: 0.9617 - val_loss: 0.1288 - val_acc: 0.9685\n",
      "Epoch 36/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9652Epoch 00035: val_loss did not improve\n",
      "719/719 [==============================] - 166s - loss: 0.0998 - acc: 0.9652 - val_loss: 0.0985 - val_acc: 0.9680\n",
      "Epoch 37/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0948 - acc: 0.9672Epoch 00036: val_loss did not improve\n",
      "719/719 [==============================] - 165s - loss: 0.0947 - acc: 0.9672 - val_loss: 0.1092 - val_acc: 0.9660\n",
      "Epoch 38/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9691Epoch 00037: val_loss did not improve\n",
      "719/719 [==============================] - 159s - loss: 0.0930 - acc: 0.9691 - val_loss: 0.1280 - val_acc: 0.9640\n",
      "Epoch 39/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9678Epoch 00038: val_loss did not improve\n",
      "719/719 [==============================] - 159s - loss: 0.0952 - acc: 0.9678 - val_loss: 0.1207 - val_acc: 0.9645\n",
      "Epoch 40/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9692Epoch 00039: val_loss improved from 0.09715 to 0.09013, saving model to /media/shreyas/OS/ML_DATA/dogsvscats/results/ft01.h5\n",
      "719/719 [==============================] - 154s - loss: 0.0894 - acc: 0.9692 - val_loss: 0.0901 - val_acc: 0.9735\n",
      "Epoch 41/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9709Epoch 00040: val_loss improved from 0.09013 to 0.08221, saving model to /media/shreyas/OS/ML_DATA/dogsvscats/results/ft01.h5\n",
      "719/719 [==============================] - 160s - loss: 0.0858 - acc: 0.9709 - val_loss: 0.0822 - val_acc: 0.9720\n",
      "Epoch 42/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0821 - acc: 0.9726Epoch 00041: val_loss did not improve\n",
      "719/719 [==============================] - 155s - loss: 0.0820 - acc: 0.9726 - val_loss: 0.1264 - val_acc: 0.9700\n",
      "Epoch 43/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9740Epoch 00042: val_loss did not improve\n",
      "719/719 [==============================] - 155s - loss: 0.0815 - acc: 0.9739 - val_loss: 0.0892 - val_acc: 0.9730\n",
      "Epoch 44/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9741Epoch 00043: val_loss did not improve\n",
      "719/719 [==============================] - 155s - loss: 0.0778 - acc: 0.9742 - val_loss: 0.0826 - val_acc: 0.9735\n",
      "Epoch 45/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9762Epoch 00044: val_loss did not improve\n",
      "719/719 [==============================] - 155s - loss: 0.0738 - acc: 0.9762 - val_loss: 0.0833 - val_acc: 0.9710\n",
      "Epoch 46/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9745Epoch 00045: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.0763 - acc: 0.9745 - val_loss: 0.1211 - val_acc: 0.9700\n",
      "Epoch 47/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9755Epoch 00046: val_loss did not improve\n",
      "719/719 [==============================] - 157s - loss: 0.0723 - acc: 0.9755 - val_loss: 0.0824 - val_acc: 0.9720\n",
      "Epoch 48/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9774Epoch 00047: val_loss did not improve\n",
      "719/719 [==============================] - 158s - loss: 0.0672 - acc: 0.9774 - val_loss: 0.0889 - val_acc: 0.9675\n",
      "Epoch 49/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9762Epoch 00048: val_loss did not improve\n",
      "719/719 [==============================] - 154s - loss: 0.0664 - acc: 0.9762 - val_loss: 0.0940 - val_acc: 0.9755\n",
      "Epoch 50/50\n",
      "718/719 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9788Epoch 00049: val_loss did not improve\n",
      "719/719 [==============================] - 156s - loss: 0.0655 - acc: 0.9788 - val_loss: 0.0834 - val_acc: 0.9685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72a6f8e208>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.fit_generator(train_b, steps_per_epoch=trn_steps, epochs=50, callbacks=callbacks,\\\n",
    "                        validation_data=valid_b, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (preds[:5])\n",
    "img = batches.filenames\n",
    "print (img[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(DATA_DIR+'test/'+img[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path + 'test_predictions.dat', preds)\n",
    "save_array(results_path + 'imagefiles.dat', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Predictions\n",
    "Lets plot -\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. Most confident correct predictions of each class\n",
    "4. Most confident incorrect predictions of each class\n",
    "5. Most uncertain labels (probabilites close to 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(validdata_path, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = val_batches.filenames\n",
    "expected_labels = val_batches.classes\n",
    "\n",
    "our_predictions = probs[:,0]\n",
    "other_predictions = np.round(probs[:,1])\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(validdata_path + img[i]) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print (\"Found %d correct labels\" % len(correct))\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#3a. The images we most confident were cats, and are actually cats\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct cats labels\" % len(correct_cats))\n",
    "most_correct_cats = np.argsort(our_predictions[correct_cats])[::-1][:n_view]\n",
    "plots_idx(correct_cats[most_correct_cats], our_predictions[correct_cats][most_correct_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3b. The images we most confident were dogs, and are actually dogs\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct dogs labels\" % len(correct_dogs))\n",
    "most_correct_dogs = np.argsort(our_predictions[correct_dogs])[:n_view]\n",
    "plots_idx(correct_dogs[most_correct_dogs], our_predictions[correct_dogs][most_correct_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4a. The images we were most confident were cats, but are actually dogs\n",
    "incorrect_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect cats\" % len(incorrect_cats))\n",
    "if len(incorrect_cats):\n",
    "    most_incorrect_cats = np.argsort(our_predictions[incorrect_cats])[::-1][:n_view]\n",
    "    plots_idx(incorrect_cats[most_incorrect_cats], our_predictions[incorrect_cats][most_incorrect_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#4b. The images we were most confident were dogs, but are actually cats\n",
    "incorrect_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect dogs\" % len(incorrect_dogs))\n",
    "if len(incorrect_dogs):\n",
    "    most_incorrect_dogs = np.argsort(our_predictions[incorrect_dogs])[:n_view]\n",
    "    plots_idx(incorrect_dogs[most_incorrect_dogs], our_predictions[incorrect_dogs][most_incorrect_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5. The most uncertain labels (ie those with probability closest to 0.5).\n",
    "most_uncertain = np.argsort(np.abs(our_predictions-0.5))\n",
    "plots_idx(most_uncertain[:n_view], our_predictions[most_uncertain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)\n",
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = load_array(results_path + 'test_predictions.dat')\n",
    "filenames = load_array(results_path + 'imagefiles.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isdog = preds[:,1]\n",
    "print (\"Raw Predictions: \"+ str(isdog[:5]))\n",
    "print (\"Mid Predictions: \"+str(isdog[(isdog<.6)&(isdog>0.4)]))\n",
    "print (\"Edge Predictions: \"+str(isdog[(isdog<0.02)&(isdog>.98)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.amax(isdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_DIR\n",
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
