{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "num_params = len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(W):\n",
    "    # reset the environment, i.e. going back to beginning\n",
    "    X = env.reset()\n",
    "    \n",
    "    # we run for 200 time steps, because 200 is the max reward we can get\n",
    "    for t in range(1,201):\n",
    "        # 0 means go left, 1 means go right\n",
    "        action = 0 if W@X < 0 else 1\n",
    "        X, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            return t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleES():\n",
    "    \"\"\"Simple Evolution strategies\"\"\"\n",
    "    def __init__(self, popsize=256):\n",
    "        self.popsize = popsize\n",
    "        self.mu = np.random.normal(0,1,4)\n",
    "        self.cov = np.full((4,4), 0.5)\n",
    "        self.best_s = self.mu\n",
    "        self.best_r = 0\n",
    "        self.first_gen = True\n",
    "    \n",
    "    def ask(self):\n",
    "        self.sols = np.random.multivariate_normal(self.mu, self.cov, self.popsize)\n",
    "        return self.sols\n",
    "        \n",
    "    def tell(self, fit_list):\n",
    "        self.fit = fit_list\n",
    "        self.best_i = np.argmax(self.fit)\n",
    "        self.best_s = self.sols[self.best_i]\n",
    "        self.best_r_ = self.fit[self.best_i]\n",
    "        \n",
    "        if self.first_gen or (self.best_r < self.best_r_): \n",
    "            self.first_gen = False\n",
    "            self.best_r = self.best_r_\n",
    "        self.mu = self.best_s\n",
    "        \n",
    "    def result(self): return self.best_s, self.best_r, self.best_r_, None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGA:\n",
    "    \"\"\"Simple Genetic Algorithm\"\"\"\n",
    "    def __init__(self, num_params, # Number of input features\n",
    "                 popsize=256,      # Number of sols that we want to generate \n",
    "                 sig_init=0.1,     # Std deviation \n",
    "                 sig_decay=0.999,  # Rate of decay for std deviation\n",
    "                 sig_lim=0.01,     # Min limit when to stop the decay\n",
    "                 elite_ratio=0.1,  # Elite popuation % to keep\n",
    "                 w_decay=0.1,      \n",
    "                 forget_best=False):\n",
    "        \n",
    "        self.num_params = num_params\n",
    "        self.popsize = popsize\n",
    "        self.sig_init = sig_init\n",
    "        self.sig_decay = sig_decay\n",
    "        self.sig_lim = sig_lim\n",
    "        self.elite_ratio = elite_ratio\n",
    "        self.w_decay = w_decay\n",
    "        self.first_gen = True\n",
    "        self.forget_best = forget_best\n",
    "        self.sig = self.sig_init\n",
    "        \n",
    "        # Initiate the size of elite population (total best sols to keep)\n",
    "        self.elite_popsize = int(self.popsize*self.elite_ratio)\n",
    "        # Initiate weights for best sols\n",
    "        self.elite_w = np.zeros((self.elite_popsize, self.num_params))\n",
    "        # Initiate fitness for best sols\n",
    "        self.elite_r = np.zeros(self.elite_popsize)\n",
    "        # Initiate parameters for best solution\n",
    "        self.best_s = np.zeros(self.num_params)\n",
    "        # Initiate best reward\n",
    "        self.best_r = 0\n",
    "        \n",
    "    def ask(self):\n",
    "        # Gaussian noise to be added after random recombination of bst sols (mating)\n",
    "        self.noise = np.random.randn(self.popsize, self.num_params)*self.sig\n",
    "        solutions = []\n",
    "        \n",
    "        \n",
    "        def mate(a, b):\n",
    "            c = np.copy(a)\n",
    "            idx = np.where(np.random.rand((c.size))>0.5)\n",
    "            # create population with parameters selected randomly from both parents\n",
    "            c[idx] = b[idx]\n",
    "            return c\n",
    "        \n",
    "        for i in range(self.popsize):\n",
    "            idx_a = np.random.choice(self.elite_popsize) # get random idx\n",
    "            idx_b = np.random.choice(self.elite_popsize) # get random idx\n",
    "            # get a child by mating two parameters with random probability\n",
    "            child = mate(self.elite_w[idx_a], self.elite_w[idx_b])\n",
    "            solutions.append(child + self.noise[i]) # add noise to params\n",
    "        \n",
    "        # convert the list to numpy array\n",
    "        solutions = np.array(solutions)\n",
    "        self.solutions = solutions\n",
    "        return solutions\n",
    "        \n",
    "    def tell(self, reward_list):\n",
    "        # assert that we have reward for every solution\n",
    "        assert (len(reward_list) == self.popsize), \"Incosistant reward size\"\n",
    "        r_list = reward_list\n",
    "        \n",
    "        \n",
    "        if self.forget_best or self.first_gen:\n",
    "            r = r_list\n",
    "            soln = self.solutions\n",
    "        \n",
    "        \n",
    "        else: \n",
    "            # add new rewards & solns to best from last genenrations.\n",
    "            r = np.concatenate([r_list,  self.elite_r])\n",
    "            soln = np.concatenate([self.solutions, self.elite_w])\n",
    "        \n",
    "        # get the indices for population with best rewards (elite population)\n",
    "        idx = r.argsort()[::-1][0:self.elite_popsize]\n",
    "        self.elite_r = r[idx]\n",
    "        self.elite_w = soln[idx]\n",
    "        \n",
    "        # best reward for this interation\n",
    "        self.best_r_ = self.elite_r[0]\n",
    "        \n",
    "        if self.first_gen or (self.best_r_ > self.best_r):\n",
    "            self.first_gen = False\n",
    "            self.best_s = np.copy(self.elite_w[0])\n",
    "            self.best_r = self.elite_r[0]\n",
    "        \n",
    "        if self.sig > self.sig_lim:\n",
    "            self.sig *= self.sig_decay\n",
    "    \n",
    "    def result(self):\n",
    "        return self.best_s, self.best_r, self.best_r_, self.sig\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNES():\n",
    "    \"\"\"Simple Natural Evolution Strategies\"\"\"\n",
    "    def __init__(self, num_params, \n",
    "                 popsize=256, \n",
    "                 sig_init=0.1, \n",
    "                 sig_decay=0.999, \n",
    "                 sig_lim=0.01, \n",
    "                 alpha = 0.1):\n",
    "        \n",
    "        self.num_params = num_params\n",
    "        self.popsize = popsize\n",
    "        self.sig_init = sig_init\n",
    "        self.sig_decay = sig_decay\n",
    "        self.sig_lim = sig_lim\n",
    "        self.first_gen = True\n",
    "        self.sig = self.sig_init\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialise the memory for solutions and best solution\n",
    "        self.solutions = np.random.randn(self.popsize, self.num_params)\n",
    "        self.best_s = np.zeros(self.num_params)\n",
    "        self.best_r = 0\n",
    "        \n",
    "    def ask(self):\n",
    "        # Sample noise from normal distribution (0,1)\n",
    "        self.noise = np.random.randn(self.popsize, self.num_params)*self.sig\n",
    "        # Jitter the solutions with gaussian noise\n",
    "        solutions = self.solutions + self.noise\n",
    "        self.solutions = solutions\n",
    "        return solutions\n",
    "        \n",
    "    def tell(self, reward_list):\n",
    "        assert (len(reward_list) == self.popsize), \"Inconsistant reward size\"\n",
    "        \n",
    "        idx = np.argmax(reward_list)\n",
    "        self.best_r_ = reward_list[idx]\n",
    "        self.best_s = self.solutions[idx]\n",
    "        \n",
    "        # Normalise the reward to gaussian distribution\n",
    "        self.r = (reward_list - np.mean(reward_list))/np.std(reward_list)\n",
    "        # Perform the parameter update (SGD)\n",
    "        # ---np.dot(self.noise.T, self.r) : This is basically weighing the Noise by reward\n",
    "        # ---self.alpha / (self.popsize*self.sig) : Get the mean for all solutions\n",
    "        self.solutions = self.solutions + self.alpha / (self.popsize*self.sig) * np.dot(self.noise.T, self.r)\n",
    "        \n",
    "        if self.first_gen or (self.best_r_ > self.best_r):\n",
    "            self.first_gen = False\n",
    "            self.best_r = self.best_r_\n",
    "        \n",
    "        if self.sig > self.sig_lim:\n",
    "            self.sig *= self.sig_decay\n",
    "            \n",
    "    def result(self):\n",
    "        return self.best_s, self.best_r, self.best_r_, self.sig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_REQUIRED_FITNESS = 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SimpleES(num_params)\n",
    "e = 0\n",
    "while e < 100:\n",
    "\n",
    "    # ask the ES to give us a set of candidate solutions\n",
    "    solutions = solver.ask()\n",
    "\n",
    "    # create an array to hold the fitness results.\n",
    "    fitness_list = np.zeros(solver.popsize)\n",
    "\n",
    "    # evaluate the fitness for each given solution.\n",
    "    for i in range(solver.popsize):\n",
    "        fitness_list[i] = evaluate(solutions[i])\n",
    "\n",
    "    # give list of fitness results back to ES\n",
    "    solver.tell(fitness_list)\n",
    "\n",
    "    # get best parameter, fitness from ES\n",
    "    best_solution, best_fitness_ever, best_fitness_current, sigma = solver.result()\n",
    "    e += 1\n",
    "    print(e, best_fitness_ever, best_fitness_current)\n",
    "    #print (best_fitness)\n",
    "    if best_fitness_ever > MY_REQUIRED_FITNESS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(W):\n",
    "    # reset the environment, i.e. going back to beginning\n",
    "    X = env.reset()\n",
    "    \n",
    "    # we run for 200 time steps, because 200 is the max reward we can get\n",
    "    for t in range(1,201):\n",
    "        env.render()\n",
    "        # 0 means go left, 1 means go right\n",
    "        action = 0 if W@X < 0 else 1\n",
    "        X, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            return t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render([8.81054195, 5.82105525, 7.1435554, 7.99971522])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
